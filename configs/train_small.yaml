run_name: "slm-small"
seed: 1337

# data
tokenizer_dir: "artifacts/tokenizer"     # must contain vocab.json, merges.txt
train_shards: ["artifacts/shards/train_*.npy"]
val_shards:   ["artifacts/shards/val_*.npy"]

# model
vocab_json: "artifacts/tokenizer/vocab.json"
max_seq_len: 256
d_model: 256
n_heads: 8
n_layers: 6
d_mlp: 1024
dropout: 0.0
attn_dropout: 0.0
mlp_dropout: 0.0
rope_theta: 10000.0
bias: false
rms_eps: 1e-5

# optimization
device: "cuda"           
dtype: "float16"         # "float32" | "float16" | "bfloat16"
batch_size: 32
grad_accum_steps: 4
lr: 3.0e-4
weight_decay: 0.1
betas: [0.9, 0.95]
eps: 1e-8
warmup_steps: 200
max_steps: 5000
grad_clip: 1.0

# checkpointing / eval
ckpt_dir: "artifacts/checkpoints/slm-small"
eval_every_steps: 250
save_every_steps: 500
log_every_steps: 50

# data prep
corpus_train: ["data/corpus_1gb_clean.txt"]  
corpus_val:   ["data/corpus_1gb_clean.txt"]  # quick sanity; replace with real val later
shard_tokens: 2_000_000                      # ~2M tokens per .npy shard
