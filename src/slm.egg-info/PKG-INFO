Metadata-Version: 2.4
Name: slm
Version: 0.1.0
Summary: From-scratch, production-style Small Language Model
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: torch>=2.2
Requires-Dist: numpy>=1.26
Requires-Dist: tqdm>=4.66
Requires-Dist: pydantic>=2.6
Requires-Dist: pyyaml>=6.0
Requires-Dist: fastapi>=0.112
Requires-Dist: uvicorn>=0.30
Requires-Dist: rich>=13.7

# SLM â€” Production-Style Small Language Model (from scratch)

This repo implements an inspectable Transformer-decoder SLM with:
- Byte-level BPE tokenizer (trainable from raw text)
- RoPE, RMSNorm, multi-head attention w/ KV cache
- Mixed precision (AMP), AdamW w/ decoupled weight decay
- Cosine LR with warmup, gradient clipping, EMA (optional)
- Streaming dataloader, checkpointing, eval (perplexity)
- Minimal FastAPI inference server + sampling utilities
- Clean config via YAML + Pydantic models
- Tests

## Quickstart
```bash
python -m venv .venv && source .venv/bin/activate
pip install -e .
python scripts/train_tokenizer.py --config configs/tokenizer.yaml
python scripts/inspect_tokenizer.py --model_dir artifacts/tokenizer
pytest
